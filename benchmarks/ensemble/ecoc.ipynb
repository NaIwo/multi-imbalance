{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from multi_imbalance.ensemble.ecoc import ECOC\n",
    "from multi_imbalance.datasets import load_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority = dict()\n",
    "minority['1czysty-cut'] = [1, 2]\n",
    "minority['2delikatne-cut'] = [1, 2]\n",
    "minority['3mocniej-cut'] = [1, 2]\n",
    "minority['4delikatne-bezover-cut'] = [1, 2]\n",
    "minority['balance-scale'] = [0]\n",
    "minority['cleveland'] = [4, 3, 2]\n",
    "minority['cleveland_v2'] = [3, 2, 1]\n",
    "minority['cmc'] = [1]\n",
    "minority['dermatology'] = [5, 3, 4, 1]\n",
    "minority['glass'] = [4, 2, 5, 3]\n",
    "minority['hayes-roth'] = [0]\n",
    "minority['new_ecoli'] = [3, 2, 4]\n",
    "minority['new_led7digit'] = [1, 4]\n",
    "minority['new_vehicle'] = [2, 0]\n",
    "minority['new_winequality-red'] = [3, 2]\n",
    "minority['new_yeast'] = [2, 3, 4, 5, 6]\n",
    "minority['thyroid-newthyroid'] = [2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_datasets()\n",
    "datasets_names = [dsn for dsn, _ in datasets.items()]\n",
    "\n",
    "binary_classifiers = ['CART', 'KNN', 'NB']\n",
    "binary_oversamplings = [None, 'globalCS', 'SMOTE']\n",
    "encodings = ['complete', 'dense', 'sparse', 'OVO', 'OVA']\n",
    "\n",
    "results_g_mean = dict()\n",
    "results_acc = dict()\n",
    "results_avg_tpr = dict()\n",
    "\n",
    "methods=[(bc,encoding, bo) for bc in binary_classifiers for encoding in encodings for bo in binary_oversamplings ]\n",
    "\n",
    "for res in (results_g_mean, results_acc, results_avg_tpr):\n",
    "    res['dataset'] = datasets_names\n",
    "    for method in methods:\n",
    "        res[method] = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1czysty-cut None CART complete\n",
      "1czysty-cut globalCS CART complete\n",
      "1czysty-cut SMOTE CART complete\n",
      "1czysty-cut None CART dense\n",
      "1czysty-cut globalCS CART dense\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_values in datasets.items():\n",
    "    X, y = dataset_values.data, dataset_values.target\n",
    "\n",
    "    for binary_classifier, encoding, oversample_binary in methods:\n",
    "\n",
    "                print(dataset_name, oversample_binary, binary_classifier, encoding)\n",
    "                acc, g_mean, avg_tpr = list(), list(), list()\n",
    "\n",
    "                for i in range(10):\n",
    "                    skf = StratifiedKFold(n_splits=5, random_state=i)\n",
    "\n",
    "                    for train_index, test_index in skf.split(X, y):\n",
    "                        X_train, X_test = X[train_index], X[test_index]\n",
    "                        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                        nn = min(np.unique(y_train, return_counts=True)[1]) - 1\n",
    "                        nn = min(nn, 3)\n",
    "                        if nn == 2: nn = 1\n",
    "\n",
    "                        ecoc = ECOC(binary_classifier=binary_classifier, oversample_binary=oversample_binary, n_neighbors=nn,\n",
    "                                  encoding=encoding)\n",
    "                        ecoc.fit(X_train, y_train)\n",
    "                        y_pred = ecoc.predict(X_test)\n",
    "\n",
    "                        g_mean.append(geometric_mean_score(y_test, y_pred, correction=0.001))\n",
    "                        acc.append(accuracy_score(y_test, y_pred))\n",
    "                        avg_tpr.append(recall_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "\n",
    "                results_g_mean[(binary_classifier, encoding, oversample_binary)].append(round(np.mean(g_mean), 3))\n",
    "                results_acc[(binary_classifier, encoding, oversample_binary)].append(round(np.mean(acc), 3))\n",
    "                results_avg_tpr[(binary_classifier, encoding, oversample_binary)].append(round(np.mean(avg_tpr), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gmean = pd.DataFrame(results_g_mean)\n",
    "df_acc = pd.DataFrame(results_acc)\n",
    "df_avg_tpr = pd.DataFrame(results_avg_tpr)\n",
    "\n",
    "for df in (df_gmean, df_acc, df_avg_tpr):\n",
    "    df.set_index('dataset')\n",
    "    df.columns = pd.MultiIndex.from_tuples(\n",
    "        [('dataset', '', '')] + [(str(bc), str(encoding), str(bo)) for bc in binary_classifiers for encoding in encodings for bo in binary_oversamplings])\n",
    "    \n",
    "import os\n",
    "import datetime\n",
    "    \n",
    "current_date = str(datetime.datetime.today())[:-7].replace(' ', '_').replace(':','_')\n",
    "\n",
    "directory='ECOC-'+current_date\n",
    "os.makedirs(directory)\n",
    "\n",
    "df_gmean.to_csv(f'./{directory}/ecoc_kfold_gmean.csv', index=False)\n",
    "df_acc.to_csv(f'./{directory}/ecoc_kfold_acc.csv', index=False)\n",
    "df_avg_tpr.to_csv(f'./{directory}/ecoc_kfold_avg_tpr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_gmean = dict()\n",
    "ranks_acc = dict()\n",
    "ranks_avg_tpr = dict()\n",
    "\n",
    "for results, ranks in zip((results_g_mean, results_acc, results_avg_tpr), (ranks_gmean, ranks_acc, ranks_avg_tpr)):\n",
    "    for ds_idx, ds in enumerate(datasets_names):\n",
    "        ranks[ds] = dict()\n",
    "        for bc in binary_classifiers:\n",
    "            ranks[ds][bc] = rankdata(\n",
    "                [-results[method][ds_idx] for method in methods if method[0]==bc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ranks_gmean=dict()\n",
    "avg_ranks_acc=dict()\n",
    "avg_ranks_avg_tpr=dict()\n",
    "\n",
    "for avg_ranks, ranks in zip((avg_ranks_gmean, avg_ranks_acc, avg_ranks_avg_tpr), (ranks_gmean, ranks_acc, ranks_avg_tpr)):\n",
    "    for bc in binary_classifiers:\n",
    "        avg_ranks[bc]=dict()\n",
    "        for method_idx, method in enumerate([m for m in methods if m[0]==bc]):\n",
    "            avg_ranks[bc][method] = round(np.mean([ranks[ds][bc][method_idx] for ds in datasets_names]),3)\n",
    "            \n",
    "for avg_ranks in (avg_ranks_gmean, avg_ranks_acc, avg_ranks_avg_tpr):\n",
    "    for bc in binary_classifiers:\n",
    "        for method in methods:\n",
    "            if method[0]==bc:\n",
    "                avg_ranks[bc][str(method[1:])] = avg_ranks[bc][method]\n",
    "                avg_ranks[bc].pop(method)\n",
    "\n",
    "df1 = pd.DataFrame(avg_ranks_gmean).T\n",
    "df2 = pd.DataFrame(avg_ranks_acc).T\n",
    "df3 = pd.DataFrame(avg_ranks_avg_tpr).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ranks_gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ranks_avg_tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean ranks in g-mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean ranks in accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean ranks in average accuracy (average recall):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
